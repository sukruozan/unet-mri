{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Activation, ReLU\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2DTranspose, Concatenate\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "import os\n",
    "import random\n",
    "#from tensorflow.keras.preprocessing.image import load_img\n",
    "import PIL\n",
    "from PIL import ImageOps, Image\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "#import time\n",
    "#import pandas as pd\n",
    "#import matplotlib\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_operation(entered_input, filters=64):\n",
    "    # Taking first input and implementing the first conv block\n",
    "    conv1 = Conv2D(filters, kernel_size = (3,3), padding = \"same\")(entered_input)\n",
    "    batch_norm1 = BatchNormalization()(conv1)\n",
    "    act1 = ReLU()(batch_norm1)\n",
    "    \n",
    "    # Taking first input and implementing the second conv block\n",
    "    conv2 = Conv2D(filters, kernel_size = (3,3), padding = \"same\")(act1)\n",
    "    batch_norm2 = BatchNormalization()(conv2)\n",
    "    act2 = ReLU()(batch_norm2)\n",
    "    \n",
    "    return act2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(entered_input, filters=64):\n",
    "    # Collect the start and end of each sub-block for normal pass and skip connections\n",
    "    enc1 = convolution_operation(entered_input, filters)\n",
    "    MaxPool1 = MaxPooling2D(strides = (2,2))(enc1)\n",
    "    return enc1, MaxPool1\n",
    "\n",
    "def decoder(entered_input, skip, filters=64):\n",
    "    # Upsampling and concatenating the essential features\n",
    "    Upsample = Conv2DTranspose(filters, (2, 2), strides=2, padding=\"same\")(entered_input)\n",
    "    Connect_Skip = Concatenate()([Upsample, skip])\n",
    "    out = convolution_operation(Connect_Skip, filters)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def U_Net(Image_Size):\n",
    "    # Take the image size and shape\n",
    "    input1 = Input(Image_Size)\n",
    "    \n",
    "    # Construct the encoder blocks\n",
    "    skip1, encoder_1 = encoder(input1, 64)\n",
    "    skip2, encoder_2 = encoder(encoder_1, 64*2)\n",
    "    skip3, encoder_3 = encoder(encoder_2, 64*4)\n",
    "    skip4, encoder_4 = encoder(encoder_3, 64*8)\n",
    "    \n",
    "    # Preparing the next block\n",
    "    conv_block = convolution_operation(encoder_4, 64*16)\n",
    "    \n",
    "    # Construct the decoder blocks\n",
    "    decoder_1 = decoder(conv_block, skip4, 64*8)\n",
    "    decoder_2 = decoder(decoder_1, skip3, 64*4)\n",
    "    decoder_3 = decoder(decoder_2, skip2, 64*2)\n",
    "    decoder_4 = decoder(decoder_3, skip1, 64)\n",
    "    \n",
    "    out = Conv2D(2, 1, padding=\"same\", activation=\"sigmoid\")(decoder_4)\n",
    "    #out = Conv2D(num_classes, 1, padding=\"same\", activation=\"sigmoid\")(decoder_4)\n",
    "    model = Model(input1, out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input shape declaration, initialize UNet Model, Show Summary\n",
    "input_shape = (256, 256, 3)\n",
    "model = U_Net(input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base folders for input and label images.Your input image names and label image names should be matched!\n",
    "# WARNING : Input image type is RGB JPEG where Label image type is GRAYSCALE PNG\n",
    "input_dir = \"Inputs\"\n",
    "target_dir = \"Labels\"\n",
    "\n",
    "img_size = (256, 256)\n",
    "num_classes = 1\n",
    "batch_size = 4\n",
    "\n",
    "input_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(input_dir, fname)\n",
    "        for fname in os.listdir(input_dir)\n",
    "        if fname.endswith(\".jpg\")\n",
    "    ]\n",
    ")\n",
    "target_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(target_dir, fname)\n",
    "        for fname in os.listdir(target_dir)\n",
    "        if fname.endswith(\".png\") and not fname.startswith(\".\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Number of samples:\", len(input_img_paths))\n",
    "\n",
    "for input_path, target_path in zip(input_img_paths[:], target_img_paths[:]):\n",
    "    print(input_path, \"|\", target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display input image #10\n",
    "display(Image.open(input_img_paths[9]))\n",
    "\n",
    "# Display auto-contrast version of corresponding target (per-pixel categories)\n",
    "img = ImageOps.autocontrast(Image.open(target_img_paths[9]))\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch generating data pipeline\n",
    "class BrainMRI(keras.utils.Sequence):\n",
    "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.input_img_paths = input_img_paths\n",
    "        self.target_img_paths = target_img_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_img_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
    "        i = idx * self.batch_size\n",
    "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
    "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
    "        for j, path in enumerate(batch_input_img_paths):\n",
    "            img = load_img(path, target_size=self.img_size)\n",
    "            x[j] = img\n",
    "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n",
    "        for j, path in enumerate(batch_target_img_paths):\n",
    "            img = load_img(path, target_size=self.img_size, color_mode=\"grayscale\")\n",
    "            tmp = np.expand_dims(img, 2)\n",
    "            y[j] = np.where( tmp > 0, 1, 0)\n",
    "            # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
    "            #y[j] -= 1\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our img paths into a training and a validation set\n",
    "val_samples = int(len(input_img_paths)*0.20)\n",
    "random.Random(1337).shuffle(input_img_paths)\n",
    "random.Random(1337).shuffle(target_img_paths)\n",
    "train_input_img_paths = input_img_paths[:-val_samples]\n",
    "train_target_img_paths = target_img_paths[:-val_samples]\n",
    "val_input_img_paths = input_img_paths[-val_samples:]\n",
    "val_target_img_paths = target_img_paths[-val_samples:]\n",
    "\n",
    "# Instantiate data Sequences for each split\n",
    "train_gen = BrainMRI(\n",
    "    batch_size, img_size, train_input_img_paths, train_target_img_paths\n",
    ")\n",
    "val_gen = BrainMRI(batch_size, img_size, val_input_img_paths, val_target_img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint( \"save_me.h5\", save_best_only=True),\n",
    "]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model, doing validation at the end of each epoch.\n",
    "epochs = 10\n",
    "history = model.fit(train_gen, epochs=epochs, validation_data=val_gen, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for all images in the validation set\n",
    "val_gen = BrainMRI(batch_size, img_size, val_input_img_paths, val_target_img_paths)\n",
    "val_preds = model.predict(val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_mask(i):\n",
    "    \"\"\"Quick utility to display a model's prediction.\"\"\"\n",
    "    mask = np.argmax(val_preds[i], axis=-1)\n",
    "    mask = np.expand_dims(mask, axis=-1)\n",
    "    img = PIL.ImageOps.autocontrast(keras.preprocessing.image.array_to_img(mask))\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results for validation image #3\n",
    "i = 2\n",
    "\n",
    "# Display input image\n",
    "display(Image.open(val_input_img_paths[i]))\n",
    "\n",
    "# Display ground-truth target mask\n",
    "img = PIL.ImageOps.autocontrast(load_img(val_target_img_paths[i]))\n",
    "display(img)\n",
    "\n",
    "# Display mask predicted by our model\n",
    "display_mask(i)  # Note that the model only sees inputs at 150x150."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
